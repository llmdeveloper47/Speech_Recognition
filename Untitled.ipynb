{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be50733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d7302e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a484adf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install librosa\n",
    "# !pip3 install transformers accelerate\n",
    "# !pip3 install soundfile\n",
    "# !pip3 install jiwer\n",
    "# !pip3 install tensorboardX\n",
    "# !pip3 install -U scikit-learn\n",
    "# !pip3 install evaluate>=0.30\n",
    "# !pip3 install gradio\n",
    "# !pip3 install transformers\n",
    "# !pip install evaluate\n",
    "# !pip3 install -U accelerate deepspeed\n",
    "# !pip3 install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de04349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shekhartanwar/opt/anaconda3/envs/HF/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-17 09:51:04,381] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pydub\n",
    "import evaluate\n",
    "from pydub import playback\n",
    "from datasets import Dataset, Audio,DatasetDict,load_from_disk\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "\n",
    "def load_audio_timestamp_dataset(dataset_path):\n",
    "    dataset = pd.read_csv(dataset_path).iloc[:,1:]\n",
    "\n",
    "    columns = dataset.columns.tolist()\n",
    "    dataset = dataset.reset_index()\n",
    "    dataset.columns = ['Index'] + columns\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_audio_path(audio_path):\n",
    "\n",
    "\n",
    "    all_folders = []\n",
    "    all_data = []\n",
    "    all_folders = os.listdir(audio_path)\n",
    "    all_folders.sort()\n",
    "    for folder in all_folders:\n",
    "\n",
    "        all_files_path = []\n",
    "\n",
    "        if folder == \".ipynb_checkpoints\" or folder == \".DS_Store\":\n",
    "            continue\n",
    "        else:\n",
    "\n",
    "            files = os.listdir(audio_path + folder + \"/\")\n",
    "            for file in files:\n",
    "                if file in \".ipynb_checkpoints\" or \".txt\" in file:\n",
    "                    continue\n",
    "                else:\n",
    "                    all_files_path.append(audio_path + folder + \"/\" + file)\n",
    "            all_files_path.sort(key = lambda x : [int(x.split(\"/\")[-1].split(\".\")[0].split(\"-\")[-1])])\n",
    "            all_folders = [folder] * len(all_files_path)\n",
    "\n",
    "\n",
    "            result = pd.DataFrame(list(zip(all_folders, all_files_path)), columns = ['file_name','audio_path'])\n",
    "            all_data.append(result)\n",
    "\n",
    "    final_result = pd.concat(all_data, axis = 0)\n",
    "    \n",
    "    final_result = final_result.reset_index()\n",
    "    final_result = final_result.drop(['index'], axis = 1)\n",
    "    final_result = final_result.reset_index()\n",
    "    final_result.columns = ['Index','file_name','audio_path']\n",
    "\n",
    "    return final_result\n",
    "\n",
    "def build_audio_dataset(directory, dataset_path):\n",
    "    \n",
    "    dataset = load_audio_timestamp_dataset(dataset_path)\n",
    "    audio_paths =  get_audio_path(directory)\n",
    "    final_dataset = pd.merge(dataset, audio_paths, how = 'inner', left_on = ['Index','file_name'], right_on =['Index','file_name'])\n",
    "    \n",
    "    final_dataset = final_dataset.drop(['Index'], axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return final_dataset\n",
    "\n",
    "\n",
    "def get_train_test_dataset(final_dataset):\n",
    "    \n",
    "    all_filenames = final_dataset['file_name'].unique().tolist()\n",
    "\n",
    "    Train_files, Test_files = train_test_split(all_filenames, test_size=0.3, random_state=412)\n",
    "    train_dataset_original = final_dataset[final_dataset.file_name.isin(Train_files)]\n",
    "    test_dataset = final_dataset[final_dataset.file_name.isin(Test_files)]\n",
    "    \n",
    "    all_filenames_train = train_dataset_original['file_name'].unique().tolist()\n",
    "\n",
    "\n",
    "    Train_files, Validation_files = train_test_split(all_filenames_train, test_size=0.3, random_state=412)\n",
    "    train_dataset = train_dataset_original[train_dataset_original['file_name'].isin(Train_files)]\n",
    "    validation_dataset = train_dataset_original[train_dataset_original['file_name'].isin(Validation_files)]\n",
    "\n",
    "    \n",
    "    \n",
    "    final_audio_dataset = DatasetDict()\n",
    "    \n",
    "    \n",
    "    return train_dataset, validation_dataset, test_dataset, final_audio_dataset\n",
    "\n",
    "    \n",
    "\n",
    "def generater_vocabulary(train_dataset):\n",
    "    \n",
    "    dataset = train_dataset.copy()\n",
    "    \n",
    "    counter = Counter()\n",
    "    \n",
    "    dataset[\"Sentences\"] = dataset[\"Sentences\"].apply(lambda x : counter.update(x.split()))\n",
    "    \n",
    "    return counter\n",
    "\n",
    "\n",
    "\n",
    "def create_hf_datset(dataset_train, dataset_validation, dataset_test, final_audio_dataset, dataset_type):\n",
    "    \n",
    "    \n",
    "    \n",
    "    final_audio_dataset[\"train\"] =  Dataset.from_dict({\"audio\":dataset_train['audio_path'], \"text\": dataset_train[\"Sentences\"]}).cast_column(\"audio\", Audio(sampling_rate = 16000))\n",
    "    final_audio_dataset[\"validation\"] =  Dataset.from_dict({\"audio\":dataset_validation['audio_path'], \"text\": dataset_validation[\"Sentences\"]}).cast_column(\"audio\", Audio(sampling_rate = 16000))\n",
    "    final_audio_dataset[\"test\"] =  Dataset.from_dict({\"audio\":dataset_test['audio_path'], \"text\": dataset_test[\"Sentences\"]}).cast_column(\"audio\", Audio(sampling_rate = 16000))\n",
    "\n",
    "            \n",
    "        \n",
    "    return final_audio_dataset\n",
    "\n",
    "\n",
    "def load_whisper_artifacts(checkpoint, final_audio_dataset):\n",
    "    \n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(checkpoint)\n",
    "    \n",
    "    processor = WhisperProcessor.from_pretrained(checkpoint, language=\"English\", task=\"transcribe\")\n",
    "\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(checkpoint, language=\"English\", task=\"transcribe\")\n",
    "\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(checkpoint)\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    device = torch.device('mps')\n",
    "    model.to(device)\n",
    "    #tokenizer.to(device)\n",
    "    \n",
    "    def prepare_dataset(batch):\n",
    "        # load and resample audio data from 48 to 16kHz\n",
    "        audio = batch[\"audio\"]\n",
    "\n",
    "        # compute log-Mel input features from input audio array \n",
    "        batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "        # encode target text to label ids \n",
    "        batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
    "        return batch\n",
    "    \n",
    "    \n",
    "    final_audio_dataset = final_audio_dataset.map(prepare_dataset, remove_columns = final_audio_dataset.column_names[\"train\"], num_proc= 4)\n",
    "    \n",
    "    \n",
    "    return feature_extractor, tokenizer, model, processor, final_audio_dataset\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "\n",
    "def master_function():\n",
    "    bucket_name = \"/Users/shekhartanwar/Documents/ASR/Data-MFA-Alignment/Data/Audio_Segments/Audio_Segments/\"\n",
    "    dataset_path = \"/Users/shekhartanwar/Documents/ASR/Data-MFA-Alignment/Data/audio_start_end.csv\" \n",
    "\n",
    "\n",
    "    final_dataset = build_audio_dataset(bucket_name, dataset_path)\n",
    "\n",
    "    \n",
    "    train_dataset, validation_dataset, test_dataset, final_audio_dataset = get_train_test_dataset(final_dataset)\n",
    "    \n",
    "    vocabulary = generater_vocabulary(train_dataset)\n",
    "    \n",
    "    \n",
    "    final_audio_dataset = create_hf_datset(train_dataset, validation_dataset, test_dataset, final_audio_dataset, \"train\")\n",
    "\n",
    "\n",
    "    \n",
    "    checkpoint = \"openai/whisper-small\"\n",
    "    feature_extractor, tokenizer, model, processor, final_audio_dataset = load_whisper_artifacts(checkpoint, final_audio_dataset)\n",
    "\n",
    "    \n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "\n",
    "\n",
    "    metric = evaluate.load(\"wer\")\n",
    "    def compute_metrics(pred):\n",
    "        pred_ids = pred.predictions\n",
    "        label_ids = pred.label_ids\n",
    "\n",
    "        # replace -100 with the pad_token_id\n",
    "        label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "        # we do not want to group tokens when computing the metrics\n",
    "        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "        wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "        return {\"wer\": wer}\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"./results\",  # change to a repo name of your choice\n",
    "    num_train_epochs= 10,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate= 1e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=1000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,   \n",
    "    )\n",
    "\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=final_audio_dataset[\"train\"],\n",
    "    eval_dataset=final_audio_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,)\n",
    "    \n",
    "    processor.save_pretrained(training_args.output_dir)\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model(\"./model/\")\n",
    "    print('\\n ----- Success\\a')\n",
    "    \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.cuda.empty_cache()\n",
    "#     master_function()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56329292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cee759fa07438b95a19c78c6d50146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/3234 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shekhartanwar/opt/anaconda3/envs/HF/lib/python3.10/site-packages/datasets/features/audio.py:303: UserWarning: \n",
      "To support 'mp3' decoding with `torchaudio>=0.12.0`, please install `ffmpeg4` system package. On Google Colab you can run:\n",
      "\n",
      "\t!add-apt-repository -y ppa:jonathonf/ffmpeg-4 && apt update && apt install -y ffmpeg\n",
      "\n",
      "and restart your runtime. Alternatively, you can downgrade `torchaudio`:\n",
      "\n",
      "\tpip install \"torchaudio<0.12\"`.\n",
      "\n",
      "Otherwise 'mp3' files will be decoded with `librosa`.\n",
      "  warnings.warn(\n",
      "/Users/shekhartanwar/opt/anaconda3/envs/HF/lib/python3.10/site-packages/datasets/features/audio.py:324: UserWarning: Decoding mp3 with `librosa` instead of `torchaudio`, decoding might be slow.\n",
      "  warnings.warn(\"Decoding mp3 with `librosa` instead of `torchaudio`, decoding might be slow.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab149d68700471489e47d30898b2e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/3234 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shekhartanwar/opt/anaconda3/envs/HF/lib/python3.10/site-packages/datasets/features/audio.py:303: UserWarning: \n",
      "To support 'mp3' decoding with `torchaudio>=0.12.0`, please install `ffmpeg4` system package. On Google Colab you can run:\n",
      "\n",
      "\t!add-apt-repository -y ppa:jonathonf/ffmpeg-4 && apt update && apt install -y ffmpeg\n",
      "\n",
      "and restart your runtime. Alternatively, you can downgrade `torchaudio`:\n",
      "\n",
      "\tpip install \"torchaudio<0.12\"`.\n",
      "\n",
      "Otherwise 'mp3' files will be decoded with `librosa`.\n",
      "  warnings.warn(\n",
      "/Users/shekhartanwar/opt/anaconda3/envs/HF/lib/python3.10/site-packages/datasets/features/audio.py:324: UserWarning: Decoding mp3 with `librosa` instead of `torchaudio`, decoding might be slow.\n",
      "  warnings.warn(\"Decoding mp3 with `librosa` instead of `torchaudio`, decoding might be slow.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3eb778ff9344b52acd5f987056160e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/3233 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shekhartanwar/opt/anaconda3/envs/HF/lib/python3.10/site-packages/datasets/features/audio.py:303: UserWarning: \n",
      "To support 'mp3' decoding with `torchaudio>=0.12.0`, please install `ffmpeg4` system package. On Google Colab you can run:\n",
      "\n",
      "\t!add-apt-repository -y ppa:jonathonf/ffmpeg-4 && apt update && apt install -y ffmpeg\n",
      "\n",
      "and restart your runtime. Alternatively, you can downgrade `torchaudio`:\n",
      "\n",
      "\tpip install \"torchaudio<0.12\"`.\n",
      "\n",
      "Otherwise 'mp3' files will be decoded with `librosa`.\n",
      "  warnings.warn(\n",
      "/Users/shekhartanwar/opt/anaconda3/envs/HF/lib/python3.10/site-packages/datasets/features/audio.py:324: UserWarning: Decoding mp3 with `librosa` instead of `torchaudio`, decoding might be slow.\n",
      "  warnings.warn(\"Decoding mp3 with `librosa` instead of `torchaudio`, decoding might be slow.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbc0632c6f14def846373d988cdfbce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/3233 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shekhartanwar/opt/anaconda3/envs/HF/lib/python3.10/site-packages/datasets/features/audio.py:303: UserWarning: \n",
      "To support 'mp3' decoding with `torchaudio>=0.12.0`, please install `ffmpeg4` system package. On Google Colab you can run:\n",
      "\n",
      "\t!add-apt-repository -y ppa:jonathonf/ffmpeg-4 && apt update && apt install -y ffmpeg\n",
      "\n",
      "and restart your runtime. Alternatively, you can downgrade `torchaudio`:\n",
      "\n",
      "\tpip install \"torchaudio<0.12\"`.\n",
      "\n",
      "Otherwise 'mp3' files will be decoded with `librosa`.\n",
      "  warnings.warn(\n",
      "/Users/shekhartanwar/opt/anaconda3/envs/HF/lib/python3.10/site-packages/datasets/features/audio.py:324: UserWarning: Decoding mp3 with `librosa` instead of `torchaudio`, decoding might be slow.\n",
      "  warnings.warn(\"Decoding mp3 with `librosa` instead of `torchaudio`, decoding might be slow.\")\n"
     ]
    }
   ],
   "source": [
    "master_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc0b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
